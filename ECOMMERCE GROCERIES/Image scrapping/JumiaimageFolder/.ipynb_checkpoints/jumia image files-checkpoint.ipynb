{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50aae15b-085a-4b49-8ffc-90e84e8b47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageScraper:\n",
    "    def __init__(self, url, download_path):\n",
    "        self.url = url\n",
    "        self.download_path = download_path\n",
    "\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    def scrape_images(self):\n",
    "        response = self.session.get(self.url).text\n",
    "\n",
    "        tree = html.fromstring(response)\n",
    "        for movie in tree.xpath('//div[@class=\"mv\"]'):\n",
    "            title = movie.findtext('.//h3/a')\n",
    "\n",
    "            image_url = \"https:\" + movie.xpath('.//img/@src')[0]\n",
    "            image_name = image_url.split('/')[-1]\n",
    "\n",
    "            self.save_image(title, image_name, image_url)\n",
    "\n",
    "    def save_image(self, movie_name, file_name, item_link):\n",
    "        response = self.session.get(item_link, stream=True)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(self.download_path, file_name), 'wb') as image_file:\n",
    "                for chunk in response.iter_content(1024):\n",
    "                    image_file.write(chunk)\n",
    "\n",
    "        print(movie_name, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7feb7-7596-4350-8f3a-79eb6ca6a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.preispirat24.com/Verbrauchsartikel/Hygiene-Artikel-127/mund-nasen-maske-3-lagig-pink-mit-nasenbuegel-ohrschlaufen-einheitsgroesse-10-stuec.html'\n",
    "soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n",
    "\n",
    "\n",
    "title=soup.find('h1',class_=\"product-info-title-desktop hidden-xs hidden-sm\").text.strip()\n",
    "description=soup.find(class_='tab-body active',itemprop=\"description\").text.strip()\n",
    "itemnumber=soup.find('span',itemprop=\"model\").text.strip()\n",
    "\n",
    "images = []\n",
    "for img in soup.select('#product_thumbnail_swiper [data-magnifier-src]'):\n",
    "    images.append('https://www.preispirat24.com/' + img['data-magnifier-src'])\n",
    "    # print('https://www.preispirat24.com/' + img['data-magnifier-src'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "        'title':title,\n",
    "        'description':description,\n",
    "        'itemnumber':itemnumber,\n",
    "        'images':[images],\n",
    "        'productlink':url\n",
    "    })\n",
    "\n",
    "df.to_csv('data.csv')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8ffd8-f29e-45ab-a13f-ed4d71e6ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in images:\n",
    "    url = element.find(\"img\").get(\"src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc80d1d-acfd-4c34-b606-14c0fb7115af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Image(url, file_path, file_name):\n",
    "    full_path = file_path + file_name + '.jpg'\n",
    "    urllib.request.urlretrieve(url, full_path)\n",
    "\n",
    "url = 'http://photo.yupoo.com/evakicks/05269e07/7bd1fc86.png'\n",
    "file_name = 'Image1'\n",
    "#!) Manually create an Image1 Folder in the same directory as this script\n",
    "\n",
    "Save_Image(url,\n",
    "          'imageFolder/',\n",
    "           file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c70ca-4dff-4623-a134-504e531a90fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, shutil\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36',\n",
    "}\n",
    "base_url = 'https://www.butterfliesofamerica.com'\n",
    "all_imgs = requests.get(base_url + '/t/Phocides_belus_a.htm', headers=headers)\n",
    "parsed_imgs = BeautifulSoup(all_imgs.text, 'html.parser')\n",
    "\n",
    "img_hrefs = [img['href'] for img in parsed_imgs.find_all('a', class_='y')]\n",
    "for img_href in img_hrefs:\n",
    "    real_img_href = img_href.replace('..', base_url)\n",
    "    image_page = requests.get(real_img_href, headers=headers)\n",
    "\n",
    "    page_soup = BeautifulSoup(image_page.text, 'html.parser')\n",
    "    source_image = page_soup.find('img')['src']\n",
    "    img_name = re.search(r'/([\\w\\-\\.]+?\\.(?:jpg|JPG))', source_image).group(1)\n",
    "\n",
    "    img = requests.get(base_url + source_image, stream=True, headers=headers)\n",
    "    with open(img_name, 'wb') as img_file:\n",
    "        shutil.copyfileobj(img.raw, img_file)\n",
    "        print(img_name, ' found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa9732-8293-4ea4-b175-d73bebc9ce4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9648665e-c010-475d-acb7-030308213d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def persist_image(folder_path:str,url:str):\n",
    "    try:\n",
    "        image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "\n",
    "    try:\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n",
    "        with open(file_path, 'wb') as f:\n",
    "            image.save(f, \"JPEG\", quality=85)\n",
    "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf834f-2857-4376-ad9e-435b2aaa5d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib2 import urlopen\n",
    "import urllib\n",
    "\n",
    "# use this image scraper from the location that \n",
    "#you want to save scraped images to\n",
    "\n",
    "def make_soup(url):\n",
    "    html = urlopen(url).read()\n",
    "    return BeautifulSoup(html)\n",
    "\n",
    "def get_images(url):\n",
    "    soup = make_soup(url)\n",
    "    #this makes a list of bs4 element tags\n",
    "    images = [img for img in soup.findAll('img')]\n",
    "    print (str(len(images)) + \"images found.\")\n",
    "    print 'Downloading images to current working directory.'\n",
    "    #compile our unicode list of image links\n",
    "    image_links = [each.get('src') for each in images]\n",
    "    for each in image_links:\n",
    "        filename=each.split('/')[-1]\n",
    "        urllib.urlretrieve(each, filename)\n",
    "    return image_links\n",
    "\n",
    "#a s# getem.py\n",
    "# python2 script to download all images in a given url\n",
    "# use: python getem.py http://url.where.images.are\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import shutil\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def make_soup(url):\n",
    "    req = urllib.request.Request(url, headers={'User-Agent' : \"Magic Browser\"}) \n",
    "    html = urllib.request.urlopen(req)\n",
    "    return BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "def get_images(url):\n",
    "    soup = make_soup(url)\n",
    "    images = [img for img in soup.findAll('img')]\n",
    "    print (str(len(images)) + \" images found.\")\n",
    "    print('Downloading images to current working directory.')\n",
    "    image_links = [each.get('src') for each in images]\n",
    "    for each in image_links:\n",
    "        try:\n",
    "            filename = each.strip().split('/')[-1].strip()\n",
    "            src = urljoin(url, each)\n",
    "            print('Getting: ' + filename)\n",
    "            response = requests.get(src, stream=True)\n",
    "            # delay to avoid corrupted previews\n",
    "            time.sleep(1)\n",
    "            with open(filename, 'wb') as out_file:\n",
    "                shutil.copyfileobj(response.raw, out_file)\n",
    "        except:\n",
    "            print('  An error occured. Continuing.')\n",
    "    print('Done.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_images('http://www.wookmark.com')tandard call looks like this\n",
    "#get_images('http://www.wookmark.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad10c6d5-7c14-4219-bffc-52a98c1969bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve as download\n",
    "\n",
    "directory = \"images\"\n",
    "Path(directory).mkdir(exist_ok=True)\n",
    "\n",
    "link = links[0]\n",
    "name = link.split(\"/\")[-1]\n",
    "\n",
    "download(f\"{base_url}/{link}\", f\"{directory}/{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e2dfcf-7b69-47eb-bfd1-0bd3870c7447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
